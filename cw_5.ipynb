{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __WSI - ćwiczenie 5.__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Sztuczne sieci neuronowe__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Treść ćwiczenia__\n",
    "\n",
    "- Celem cwiczenia jest implementacja perceptronu wielowarstwowego oraz wybranego algorytmu\n",
    "optymalizacji gradientowej z algorytmem propagacji wstecznej.\n",
    "- Nastepnie nalezy wytrenowac perceptron wielowarstwowy do klasyfikacji zbioru danych wine\n",
    "(https://archive.ics.uci.edu/ml/datasets/wine). Zbiór ten dostepny jest w pakiecie scikitlearn\n",
    "(sklearn.datasets.load wine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score, precision_recall_curve, auc, RocCurveDisplay, PrecisionRecallDisplay, recall_score, precision_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from seaborn import heatmap\n",
    "import plotly.express as px\n",
    "from math import log, inf, e, tanh, sqrt\n",
    "from sklearn.utils import resample, shuffle\n",
    "import unittest\n",
    "\n",
    "RNG = np.random.default_rng()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cele: \n",
    "- perceptron wielowarstwowy, implementacja ze zmienną ilością warstw głębokich oraz zmienną ilością długości wektora neuronów\n",
    "- kilka algorytmów optymalizacji wag sieci (gradient prosty, SGD, algorytm ewolucyjny??)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zadania:\n",
    "1. model sieci\n",
    "2. propagacja wsteczna\n",
    "3. optymalizacja wag"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Weight matrix per layer:__\n",
    "\n",
    "$$\n",
    "\\theta^{l}=\n",
    "\\left[\\begin{array}{ccc}\n",
    "\\omega_{1,1}& \\cdots&\\omega_{1,k+1}\\\\\n",
    "\\vdots&\\ddots&\\vdots\\\\\n",
    "\\omega_{n,1}&\\cdots&\\omega_{n,k+1}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "where  $ \\omega_{i,j} $ is the $j$-th weight of the $i$-th neuron (in layer $l$), and $ \\omega_{i,k+1} $ is its bias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Matrix of layers:__\n",
    "\n",
    "$$\n",
    "\\Theta=\n",
    "\\left[\\begin{array}{ccc}\n",
    "\\theta^{1}& \\cdots&\\theta^{\\lambda}\\\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "where  $ \\theta^{\\lambda} $ is the output layer "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Input data vector:__\n",
    "\n",
    "$$\n",
    "y^0=\\left[\\begin{array}{ccc}\n",
    "x^T& 1\n",
    "\\end{array}\\right]^T\n",
    "$$\n",
    "it is extended by 1 to allow easier multiplication"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Opertation of a single neuron:__\n",
    "\n",
    "$$\n",
    "y^l_i=\\psi(\\theta^l_i y^{l-1})\n",
    "$$\n",
    "\n",
    "$\\psi$ is the neuron activation function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Output layer:__\n",
    "\n",
    "$$\n",
    "f_i(x, \\Theta)=\\theta^\\lambda_i y^\\lambda\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Backpropagation__\n",
    "$$\n",
    "\\frac{de}{ds^l_i}=\\frac{de}{dy^l_i}\\frac{∂\\psi^l(s^l_i)}{∂s^l_i}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{de}{dy^l_i}=\\sum_{\\gamma} \\frac{de}{ds^{l+1}_\\gamma}\\theta^{l+1}_{\\gamma,i} \n",
    "$$\n",
    "For the last layer this can be calculated immidiately:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{∂e}{∂\\theta^l_{i,j}}=\\frac{de}{ds^l_i}y^{l-1}_j= \\frac{de}{dy^l_i} \\frac{∂\\psi^l(s^l_i)}{∂s^l_i}y^{l-1}_j\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest:\n",
    "$$\n",
    "\\frac{∂e}{∂\\theta^l_{i,j}}= \\left( \\sum_{\\gamma} \\frac{de}{ds^{l+1}_\\gamma}\\theta^{l+1}_{\\gamma,i} \\right) \\frac{∂\\psi^l(s^l_i)}{∂s^l_i}y^{l-1}_j\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in order to calculate all the derivatives we need to first calculate all derivatives of neuron input sums"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Default weigths initialization:__\n",
    "\n",
    "todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP():\n",
    "    \"\"\"\n",
    "    todo fully-connected?\n",
    "\n",
    "    Attribubtes:\n",
    "        _layers: \n",
    "\n",
    "    Methods:\n",
    "        fit:\n",
    "    \"\"\"\n",
    "    def __init__(self, dimensions:list, activations:list, derivatives:list, feature_number:int) -> None:\n",
    "        \"\"\"\n",
    "        todo\n",
    "\n",
    "        Args:\n",
    "            dimensions: starting from first hidden layer\n",
    "            activations: last actvation function should be linear if a basic MLP is being modeled\n",
    "\n",
    "        Returns:\n",
    "            MLP object\n",
    "\n",
    "        Raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        # todo assertions\n",
    "\n",
    "        self._layers = [np.empty((dimensions[0], feature_number + 1))] + \\\n",
    "                       [np.empty((dimensions[i+1], dimensions[i]+1)) for i in range(len(dimensions)-1)]\n",
    "        self._activations = activations\n",
    "\n",
    "    def initialize_weights(self, strategy='default'):\n",
    "        # todo strategies\n",
    "        if strategy == 'default':\n",
    "            for layer in self._layers[:-1]:\n",
    "                size = layer.shape[1]\n",
    "                size_sqrt = sqrt(size)\n",
    "                with np.nditer(layer, op_flags=['writeonly']) as it:\n",
    "                    for w in it:\n",
    "                        w[...] = RNG.uniform(-1/size_sqrt, 1/size_sqrt)\n",
    "            self._layers[-1].fill(0)\n",
    "\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def feed_forward(self, input_vector):\n",
    "        # todo description\n",
    "        # this returns\n",
    "        all_outputs = [[]]\n",
    "        current_layer_outputs = input_vector\n",
    "        for layer, activate in zip(self._layers, self._activations):\n",
    "            current_layer_outputs = current_layer_outputs + [1]\n",
    "            current_layer_outputs = [activate(np.matmul(weights, current_layer_outputs)) for weights in layer]\n",
    "            all_outputs.append(current_layer_outputs)\n",
    "        return all_outputs\n",
    "\n",
    "    def predict_single_data_point(self, input_vector):\n",
    "        ...\n",
    "\n",
    "    def backprop(self, input_vecor, true_output_vector, loss_func_derivative):\n",
    "        all_outputs = self.feed_forward(input_vecor)\n",
    "        deriv_input_sum = ...\n",
    "        deriv_loss = loss_func_derivative(output_vector, true_vector)\n",
    "\n",
    "    def predict(self, data):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "......\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 0.004s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x132fe028650>"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestMLP(unittest.TestCase):\n",
    "\n",
    "    def test_init_1_neuron(self):\n",
    "        mlp = MLP([1], [], [], 1)\n",
    "        self.assertEqual(len(mlp._layers), 1)\n",
    "        self.assertEqual(mlp._layers[0].shape, (1, 2))\n",
    "\n",
    "    def test_init_multi_neuron(self):\n",
    "        mlp = MLP([5, 3, 11], [], [], 15)\n",
    "        self.assertEqual(len(mlp._layers), 3)\n",
    "        self.assertEqual(mlp._layers[0].shape, (5, 16))\n",
    "        self.assertEqual(mlp._layers[1].shape, (3, 6))\n",
    "        self.assertEqual(mlp._layers[2].shape, (11, 4))\n",
    "\n",
    "    def test_1_feature_feed_forward_1_neuron(self):\n",
    "        mlp = MLP([1], [lambda x: x], [], 1)\n",
    "        mlp._layers[0] = np.array([[3, 2]])\n",
    "        self.assertEqual(mlp.feed_forward([3])[-1], [11])\n",
    "\n",
    "    def test_multi_features_feed_forward_mutli_neuron(self):\n",
    "        mlp = MLP([2, 3, 2], [lambda x: x, lambda x: 2*x, lambda x: x], [], 2)\n",
    "        mlp._layers[0].fill(1)\n",
    "        mlp._layers[1].fill(1)\n",
    "        mlp._layers[2].fill(1)\n",
    "        self.assertEqual(mlp.feed_forward([2,  3])[-1], [79, 79])\n",
    "\n",
    "    def test_initialize_weigths_default(self):\n",
    "        mlp = MLP([2, 3, 2], [lambda x: x, lambda x: 2*x, lambda x: x], [], 2)\n",
    "        mlp.initialize_weights(strategy='default')\n",
    "        for layer in mlp._layers[:-1]:\n",
    "            self.assertTrue(((layer > -1) & (layer < 1)).all())\n",
    "        self.assertTrue((mlp._layers[-1] == 0).all())\n",
    "\n",
    "    def test_backprop(self):\n",
    "        mlp = MLP([2, 3, 2], [lambda x: x, lambda x: 2*x, lambda x: x], [lambda x: 1, lambda x: 2, lambda x: 1], 2)\n",
    "        mlp._layers[0].fill(1)\n",
    "        mlp._layers[1].fill(1)\n",
    "        mlp._layers[2].fill(1)\n",
    "        self.assertEqual(mlp.feed_forward([2,  3])[-1], [79, 79])\n",
    "\n",
    "    \n",
    "\n",
    "unittest.main(argv=[''],  exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data=[[x, x**2] for x in RNG.uniform(-1, 1, 100)], columns=['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP([10, 1], [tanh, lambda x: x], [1, 1], 1)\n",
    "mlp.initialize_weights(strategy='default')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1793df1d2301171ba0b40f65791b7f70118724e366e1aedbfbefba442cbae2c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
