{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __WSI - ćwiczenie 5.__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Sztuczne sieci neuronowe__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Treść ćwiczenia__\n",
    "\n",
    "- Celem cwiczenia jest implementacja perceptronu wielowarstwowego oraz wybranego algorytmu\n",
    "optymalizacji gradientowej z algorytmem propagacji wstecznej.\n",
    "- Nastepnie nalezy wytrenowac perceptron wielowarstwowy do klasyfikacji zbioru danych wine\n",
    "(https://archive.ics.uci.edu/ml/datasets/wine). Zbiór ten dostepny jest w pakiecie scikitlearn\n",
    "(sklearn.datasets.load wine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score, precision_recall_curve, auc, RocCurveDisplay, PrecisionRecallDisplay, recall_score, precision_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from seaborn import heatmap\n",
    "import plotly.express as px\n",
    "from math import log, inf, e, tanh, sqrt\n",
    "from sklearn.utils import resample, shuffle\n",
    "import unittest\n",
    "\n",
    "RNG = np.random.default_rng()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cele: \n",
    "- perceptron wielowarstwowy, implementacja ze zmienną ilością warstw głębokich oraz zmienną ilością długości wektora neuronów\n",
    "- kilka algorytmów optymalizacji wag sieci (gradient prosty, SGD, algorytm ewolucyjny??)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zadania:\n",
    "1. model sieci\n",
    "2. propagacja wsteczna\n",
    "3. optymalizacja wag"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Weight matrix per layer:__\n",
    "\n",
    "$$\n",
    "\\theta^{l}=\n",
    "\\left[\\begin{array}{ccc}\n",
    "\\omega_{1,1}& \\cdots&\\omega_{1,k+1}\\\\\n",
    "\\vdots&\\ddots&\\vdots\\\\\n",
    "\\omega_{n,1}&\\cdots&\\omega_{n,k+1}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "where  $ \\omega_{i,j} $ is the $j$-th weight of the $i$-th neuron (in layer $l$), and $ \\omega_{i,k+1} $ is its bias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Matrix of layers:__\n",
    "\n",
    "$$\n",
    "\\Theta=\n",
    "\\left[\\begin{array}{ccc}\n",
    "\\theta^{1}& \\cdots&\\theta^{\\lambda}\\\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "where  $ \\theta^{\\lambda} $ is the output layer "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Input data vector:__\n",
    "\n",
    "$$\n",
    "y^0=\\left[\\begin{array}{ccc}\n",
    "x^T& 1\n",
    "\\end{array}\\right]^T\n",
    "$$\n",
    "it is extended by 1 to allow easier multiplication"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Opertation of a single neuron:__\n",
    "\n",
    "$$\n",
    "y^l_i=\\psi(\\theta^l_i y^{l-1})\n",
    "$$\n",
    "\n",
    "$\\psi$ is the neuron activation function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Output layer:__\n",
    "\n",
    "$$\n",
    "f_i(x, \\Theta)=\\theta^\\lambda_i y^\\lambda\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Backpropagation__\n",
    "$$\n",
    "\\frac{de}{ds^l_i}=\\frac{de}{dy^l_i}\\frac{∂\\psi^l(s^l_i)}{∂s^l_i}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{de}{dy^l_i}=\\sum_{\\gamma} \\frac{de}{ds^{l+1}_\\gamma}\\theta^{l+1}_{\\gamma,i} \n",
    "$$\n",
    "For the last layer this can be calculated immidiately:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{∂e}{∂\\theta^l_{i,j}}=\\frac{de}{ds^l_i}y^{l-1}_j= \\frac{de}{dy^l_i} \\frac{∂\\psi^l(s^l_i)}{∂s^l_i}y^{l-1}_j\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest:\n",
    "$$\n",
    "\\frac{∂e}{∂\\theta^l_{i,j}}= \\left( \\sum_{\\gamma} \\frac{de}{ds^{l+1}_\\gamma}\\theta^{l+1}_{\\gamma,i} \\right) \\frac{∂\\psi^l(s^l_i)}{∂s^l_i}y^{l-1}_j\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in order to calculate all the derivatives we need to first calculate all derivatives of neuron input sums"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Default weigths initialization:__\n",
    "\n",
    "todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP():\n",
    "    \"\"\"\n",
    "    todo fully-connected?\n",
    "\n",
    "    Attribubtes:\n",
    "        _layers: \n",
    "\n",
    "    Methods:\n",
    "        fit:\n",
    "    \"\"\"\n",
    "    def __init__(self, dimensions:list, activations:list, derivatives:list, feature_number:int) -> None:\n",
    "        \"\"\"\n",
    "        todo\n",
    "\n",
    "        Args:\n",
    "            dimensions: starting from first hidden layer\n",
    "            activations: last actvation function should be linear if a basic MLP is being modeled\n",
    "\n",
    "        Returns:\n",
    "            MLP object\n",
    "\n",
    "        Raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        # todo assertions\n",
    "\n",
    "        self._layers = [np.empty((dimensions[0], feature_number + 1))] + \\\n",
    "                       [np.empty((dimensions[i+1], dimensions[i]+1)) for i in range(len(dimensions)-1)]\n",
    "        self._activations = activations\n",
    "        self._derivatives = derivatives\n",
    "\n",
    "    def initialize_weights(self, strategy='default'):\n",
    "        # todo strategies\n",
    "        if strategy == 'default':\n",
    "            for layer in self._layers[:-1]:\n",
    "                size = layer.shape[1]\n",
    "                size_sqrt = sqrt(size)\n",
    "                with np.nditer(layer, op_flags=['writeonly']) as it:\n",
    "                    for w in it:\n",
    "                        w[...] = RNG.uniform(-1/size_sqrt, 1/size_sqrt)\n",
    "            self._layers[-1].fill(0.01) \n",
    "            # todo 0!!!!!!!!!!!\n",
    "\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def feed_forward(self, input_vector):\n",
    "        # todo description\n",
    "        # this returns a vector consisting of tuples \n",
    "        # of single neuron input sum and effective activated output \n",
    "        all_inputs_outputs = []\n",
    "        current_layer_activated_outputs = input_vector\n",
    "        for layer, activate in zip(self._layers, self._activations):\n",
    "            current_layer_activated_outputs = current_layer_activated_outputs + [1]\n",
    "            current_layer_input_sums = [np.matmul(weights, current_layer_activated_outputs) for weights in layer]\n",
    "            current_layer_activated_outputs = [activate(s) for s in current_layer_input_sums]\n",
    "            current_layer_pairs = np.array([current_layer_input_sums, current_layer_activated_outputs]).T\n",
    "            all_inputs_outputs.append(current_layer_pairs)\n",
    "        return all_inputs_outputs\n",
    "\n",
    "    def predict_single_data_point(self, input_vector):\n",
    "        return self.feed_forward(input_vector)[-1][:, 1].tolist()\n",
    "\n",
    "    def backprop(self, input_vector, true_output_vector, loss_func_derivative):\n",
    "        \n",
    "        gradient_estimate = []\n",
    "        \n",
    "        all_inputs_outputs = self.feed_forward(input_vector)\n",
    "        all_partial_deriv_input_sums = [[deriv(s) for s in layer[:, 0]] for layer, deriv in zip(all_inputs_outputs, self._derivatives)]\n",
    "        \n",
    "        # configure for output layer\n",
    "\n",
    "        # calculate loss derivative values\n",
    "        total_deriv_output_sums = [loss_func_derivative(output, true_value) for output, true_value in zip(all_inputs_outputs[-1][:,1], true_output_vector)]\n",
    "\n",
    "        # for each layer\n",
    "        layer_idx = -1\n",
    "        partial_deriv_input_sums = all_partial_deriv_input_sums[layer_idx]\n",
    "        total_deriv_input_sums = np.multiply(total_deriv_output_sums, partial_deriv_input_sums)\n",
    "        np.array([])\n",
    "        gradient_estimate.insert(0, )\n",
    "        \n",
    "        print(total_deriv_input_sums)\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, data):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "...F.\n",
      "======================================================================\n",
      "FAIL: test_initialize_weigths_default (__main__.TestMLP)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dziew\\AppData\\Local\\Temp\\ipykernel_13508\\3794067282.py\", line 32, in test_initialize_weigths_default\n",
      "    self.assertTrue((mlp._layers[-1] == 0).all())\n",
      "AssertionError: False is not true\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 0.010s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x2035110cbb0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestMLP(unittest.TestCase):\n",
    "\n",
    "    def test_init_1_neuron(self):\n",
    "        mlp = MLP([1], [], [], 1)\n",
    "        self.assertEqual(len(mlp._layers), 1)\n",
    "        self.assertEqual(mlp._layers[0].shape, (1, 2))\n",
    "\n",
    "    def test_init_multi_neuron(self):\n",
    "        mlp = MLP([5, 3, 11], [], [], 15)\n",
    "        self.assertEqual(len(mlp._layers), 3)\n",
    "        self.assertEqual(mlp._layers[0].shape, (5, 16))\n",
    "        self.assertEqual(mlp._layers[1].shape, (3, 6))\n",
    "        self.assertEqual(mlp._layers[2].shape, (11, 4))\n",
    "\n",
    "    def test_1_feature_feed_forward_1_neuron(self):\n",
    "        mlp = MLP([1], [lambda x: x], [], 1)\n",
    "        mlp._layers[0] = np.array([[3, 2]])\n",
    "        self.assertEqual(mlp.predict_single_data_point([3]), [11.])\n",
    "\n",
    "    def test_multi_features_feed_forward_mutli_neuron(self):\n",
    "        mlp = MLP([2, 3, 2], [lambda x: x, lambda x: 2*x, lambda x: x], [], 2)\n",
    "        mlp._layers[0].fill(1)\n",
    "        mlp._layers[1].fill(1)\n",
    "        mlp._layers[2].fill(1)\n",
    "        self.assertEqual(mlp.predict_single_data_point([2,  3]), [79., 79.])\n",
    "\n",
    "    def test_initialize_weigths_default(self):\n",
    "        mlp = MLP([2, 3, 2], [lambda x: x, lambda x: 2*x, lambda x: x], [], 2)\n",
    "        mlp.initialize_weights(strategy='default')\n",
    "        for layer in mlp._layers[:-1]:\n",
    "            self.assertTrue(((layer > -1) & (layer < 1)).all())\n",
    "        self.assertTrue((mlp._layers[-1] == 0).all())\n",
    "\n",
    "    # def test_backprop(self):\n",
    "    #     mlp = MLP([2, 3, 2], [lambda x: x, lambda x: 2*x, lambda x: x], [lambda x: 1, lambda x: 2, lambda x: 1], 2)\n",
    "    #     mlp._layers[0].fill(1)\n",
    "    #     mlp._layers[1].fill(1)\n",
    "    #     mlp._layers[2].fill(1)\n",
    "    #     self.assertEqual(mlp.predict_single_data_point([2,  3]), [79, 79])\n",
    "\n",
    "    \n",
    "\n",
    "unittest.main(argv=[''],  exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data=[[x, x**2] for x in RNG.uniform(-1, 1, 100)], columns=['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP([10, 5], [lambda x: (x**2)-0.5, lambda x: x], [lambda x: 2*x, lambda x: x], 1)\n",
    "mlp.initialize_weights(strategy='default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.43822438 3.43822438 3.43822438 3.43822438 3.43822438]\n"
     ]
    }
   ],
   "source": [
    "mlp.backprop([12], [1, 1, 1, 1, 1], lambda t, v: (t - v)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 9])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.multiply([1,2,3], [1,2,3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp_env",
   "language": "python",
   "name": "bootcamp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1793df1d2301171ba0b40f65791b7f70118724e366e1aedbfbefba442cbae2c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
