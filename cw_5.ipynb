{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __WSI - ćwiczenie 5.__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Sztuczne sieci neuronowe__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Treść ćwiczenia__\n",
    "\n",
    "- Celem cwiczenia jest implementacja perceptronu wielowarstwowego oraz wybranego algorytmu\n",
    "optymalizacji gradientowej z algorytmem propagacji wstecznej.\n",
    "- Nastepnie nalezy wytrenowac perceptron wielowarstwowy do klasyfikacji zbioru danych wine\n",
    "(https://archive.ics.uci.edu/ml/datasets/wine). Zbiór ten dostepny jest w pakiecie scikitlearn\n",
    "(sklearn.datasets.load wine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, confusion_matrix, roc_curve, roc_auc_score, precision_recall_curve, auc, RocCurveDisplay, PrecisionRecallDisplay, recall_score, precision_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from seaborn import heatmap\n",
    "import plotly.express as px\n",
    "from math import log, inf, e, tanh, sqrt\n",
    "from sklearn.utils import resample, shuffle\n",
    "import unittest\n",
    "\n",
    "RNG = np.random.default_rng()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cele: \n",
    "- perceptron wielowarstwowy, implementacja ze zmienną ilością warstw głębokich oraz zmienną ilością długości wektora neuronów\n",
    "- kilka algorytmów optymalizacji wag sieci (gradient prosty, SGD, algorytm ewolucyjny??)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zadania:\n",
    "1. model sieci\n",
    "2. propagacja wsteczna\n",
    "3. optymalizacja wag"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Weight matrix per layer:__\n",
    "\n",
    "$$\n",
    "\\theta^{l}=\n",
    "\\left[\\begin{array}{ccc}\n",
    "\\omega_{1,1}& \\cdots&\\omega_{1,k+1}\\\\\n",
    "\\vdots&\\ddots&\\vdots\\\\\n",
    "\\omega_{n,1}&\\cdots&\\omega_{n,k+1}\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "where  $ \\omega_{i,j} $ is the $j$-th weight of the $i$-th neuron (in layer $l$), and $ \\omega_{i,k+1} $ is its bias"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Matrix of layers:__\n",
    "\n",
    "$$\n",
    "\\Theta=\n",
    "\\left[\\begin{array}{ccc}\n",
    "\\theta^{1}& \\cdots&\\theta^{\\lambda}\\\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "where  $ \\theta^{\\lambda} $ is the output layer "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Input data vector:__\n",
    "\n",
    "$$\n",
    "y^0=\\left[\\begin{array}{ccc}\n",
    "x^T& 1\n",
    "\\end{array}\\right]^T\n",
    "$$\n",
    "it is extended by 1 to allow easier multiplication"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Opertation of a single neuron:__\n",
    "\n",
    "$$\n",
    "y^l_i=\\psi(\\theta^l_i y^{l-1})\n",
    "$$\n",
    "\n",
    "$\\psi$ is the neuron activation function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Output layer:__\n",
    "\n",
    "$$\n",
    "f_i(x, \\Theta)=\\theta^\\lambda_i y^\\lambda\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Backpropagation__\n",
    "$$\n",
    "\\frac{de}{ds^l_i}=\\frac{de}{dy^l_i}\\frac{∂\\psi^l(s^l_i)}{∂s^l_i}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{de}{dy^l_i}=\\sum_{\\gamma} \\frac{de}{ds^{l+1}_\\gamma}\\theta^{l+1}_{\\gamma,i} \n",
    "$$\n",
    "For the last layer this can be calculated immidiately:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{∂e}{∂\\theta^l_{i,j}}=\\frac{de}{ds^l_i}y^{l-1}_j= \\frac{de}{dy^l_i} \\frac{∂\\psi^l(s^l_i)}{∂s^l_i}y^{l-1}_j\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest:\n",
    "$$\n",
    "\\frac{∂e}{∂\\theta^l_{i,j}}= \\left( \\sum_{\\gamma} \\frac{de}{ds^{l+1}_\\gamma}\\theta^{l+1}_{\\gamma,i} \\right) \\frac{∂\\psi^l(s^l_i)}{∂s^l_i}y^{l-1}_j\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in order to calculate all the derivatives we need to first calculate all derivatives of neuron input sums"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Default weigths initialization:__\n",
    "\n",
    "todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP():\n",
    "    \"\"\"\n",
    "    todo fully-connected?\n",
    "\n",
    "    Attribubtes:\n",
    "        _layers: \n",
    "\n",
    "    Methods:\n",
    "        fit:\n",
    "    \"\"\"\n",
    "    def __init__(self, dimensions:list, activations:list, derivatives:list, feature_number:int) -> None:\n",
    "        \"\"\"\n",
    "        todo\n",
    "\n",
    "        Args:\n",
    "            dimensions: starting from first hidden layer\n",
    "            activations: last actvation function should be linear if a basic MLP is being modeled\n",
    "\n",
    "        Returns:\n",
    "            MLP object\n",
    "\n",
    "        Raises:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        # todo assertions\n",
    "\n",
    "        self._layers = np.array([np.empty((dimensions[0], feature_number + 1))] + \\\n",
    "                                [np.empty((dimensions[i+1], dimensions[i]+1)) for i in range(len(dimensions)-1)], \n",
    "                                dtype=object)\n",
    "        self._activations = activations\n",
    "        self._derivatives = derivatives\n",
    "\n",
    "    def initialize_weights(self, strategy='default'):\n",
    "        # todo strategies\n",
    "        if strategy == 'default':\n",
    "            for layer in self._layers[:-1]:\n",
    "                size = layer.shape[1]\n",
    "                size_sqrt = sqrt(size)\n",
    "                with np.nditer(layer, op_flags=['writeonly']) as it:\n",
    "                    for w in it:\n",
    "                        w[...] = RNG.uniform(-1/size_sqrt, 1/size_sqrt)\n",
    "            self._layers[-1].fill(0)\n",
    "        if strategy == 'ones':\n",
    "            for layer in self._layers:\n",
    "                layer.fill(1)\n",
    "\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def feed_forward(self, input_vector):\n",
    "        # todo description\n",
    "        # this returns a vector consisting of tuples \n",
    "        # of single neuron input sum and effective activated output \n",
    "        all_inputs_outputs = []\n",
    "        current_layer_activated_outputs = input_vector\n",
    "        for layer, activate in zip(self._layers, self._activations):\n",
    "            current_layer_activated_outputs = current_layer_activated_outputs + [1]\n",
    "            current_layer_input_sums = [np.matmul(weights, current_layer_activated_outputs) for weights in layer]\n",
    "            current_layer_activated_outputs = [activate(s) for s in current_layer_input_sums]\n",
    "            current_layer_pairs = np.array([current_layer_input_sums, current_layer_activated_outputs]).T\n",
    "            all_inputs_outputs.append(current_layer_pairs)\n",
    "        return all_inputs_outputs\n",
    "\n",
    "    def propagate(self, input_vector):\n",
    "        return self.feed_forward(input_vector)[-1][:, 1].tolist()\n",
    "\n",
    "    def backprop(self, input_vector, true_output_vector, loss_func_derivative):\n",
    "        \n",
    "        gradient_estimate = []\n",
    "        \n",
    "        all_inputs_outputs = self.feed_forward(input_vector)\n",
    "        all_partial_deriv_input_sums = [[deriv(s) for s in layer[:, 0]] for layer, deriv in zip(all_inputs_outputs, self._derivatives)]\n",
    "        \n",
    "        # configure for output layer\n",
    "        # calculate loss derivative values\n",
    "        total_deriv_outputs = [loss_func_derivative(output, true_value) for output, true_value in zip(all_inputs_outputs[-1][:,1], true_output_vector)]\n",
    "        # all outputs appended with 1 (bias) at each layer\n",
    "        all_outputs_with_input_vector = [input_vector + [1]] + [layer[:,1].tolist() + [1] for layer in all_inputs_outputs]\n",
    "        # for each layer\n",
    "        layer_idx = -1\n",
    "        for layer_idx in range(len(self._layers)-1, -1, -1):\n",
    "            partial_deriv_input_sums = all_partial_deriv_input_sums[layer_idx]\n",
    "            total_deriv_input_sums = np.multiply(total_deriv_outputs, partial_deriv_input_sums)\n",
    "            # calculate estimated gradient for layer \n",
    "            layer_gradient = np.matmul(total_deriv_input_sums[np.newaxis].T, np.array(all_outputs_with_input_vector[layer_idx])[np.newaxis])\n",
    "            gradient_estimate.insert(0, layer_gradient)\n",
    "            \n",
    "            # prepare for next layer \n",
    "            if not layer_idx == 0:\n",
    "                # shorten the arrays by 1 to avoid calculating derivatives for biases\n",
    "                total_deriv_outputs = [np.multiply(total_deriv_input_sums, neuron_weights).sum() for neuron_weights in self._layers[layer_idx][:,:-1].T]\n",
    "\n",
    "        return np.array(gradient_estimate, dtype=object)\n",
    "\n",
    "    def BGD_step(self, data, target_label, learning_rate):\n",
    "        gradient_sum = np.shape(self._layers)\n",
    "        for _, row in data.iterrows():\n",
    "            gradient_sum = gradient_sum + self.backprop(row.drop([target_label]).tolist(), \n",
    "                                                         [row[target_label]], \n",
    "                                                         lambda v, t: 2*(v-t))\n",
    "        avg_gradient = gradient_sum / data.shape[0]\n",
    "        self._layers = self._layers - (avg_gradient * learning_rate)\n",
    "    \n",
    "    def SGD_step(self, data, mini_batch_size):\n",
    "        ...\n",
    "\n",
    "    def train(self, train_data, test_data, target_label, learning_rate, strategy='BGD', epoch_number=100, mini_batch_size=0.1, monitor=False):\n",
    "        errors = []\n",
    "        for _ in range(epoch_number):\n",
    "            if strategy == 'BGD':\n",
    "                self.BGD_step(train_data, target_label, learning_rate)\n",
    "            if strategy == 'SGD':\n",
    "                ...\n",
    "            if monitor:\n",
    "                pred = self.predict(test_data.drop(target_label, axis=1)).apply(lambda row: row[0])\n",
    "                mse = mean_squared_error(test_data[target_label], pred)\n",
    "                errors.append(mse)\n",
    "        return errors\n",
    "\n",
    "            \n",
    "\n",
    "    def predict(self, data):\n",
    "        return data.apply(lambda row : self.propagate(row.tolist()), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".......\n",
      "----------------------------------------------------------------------\n",
      "Ran 7 tests in 0.004s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x210803d5090>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TestMLP(unittest.TestCase):\n",
    "\n",
    "    def test_init_1_neuron(self):\n",
    "        mlp = MLP([1], [], [], 1)\n",
    "        self.assertEqual(len(mlp._layers), 1)\n",
    "        self.assertEqual(mlp._layers[0].shape, (1, 2))\n",
    "\n",
    "    def test_init_multi_neuron(self):\n",
    "        mlp = MLP([5, 3, 11], [], [], 15)\n",
    "        self.assertEqual(len(mlp._layers), 3)\n",
    "        self.assertEqual(mlp._layers[0].shape, (5, 16))\n",
    "        self.assertEqual(mlp._layers[1].shape, (3, 6))\n",
    "        self.assertEqual(mlp._layers[2].shape, (11, 4))\n",
    "\n",
    "    def test_1_feature_feed_forward_1_neuron(self):\n",
    "        mlp = MLP([1], [lambda x: x], [], 1)\n",
    "        mlp._layers[0] = np.array([[3, 2]])\n",
    "        self.assertEqual(mlp.propagate([3]), [11.])\n",
    "\n",
    "    def test_multi_features_feed_forward_mutli_neuron(self):\n",
    "        mlp = MLP([2, 3, 2], [lambda x: x, lambda x: 2*x, lambda x: x], [], 2)\n",
    "        mlp.initialize_weights('ones')\n",
    "        self.assertEqual(mlp.propagate([2,  3]), [79., 79.])\n",
    "\n",
    "    def test_initialize_weigths_default(self):\n",
    "        mlp = MLP([2, 3, 2], [lambda x: x, lambda x: 2*x, lambda x: x], [], 2)\n",
    "        mlp.initialize_weights(strategy='default')\n",
    "        for layer in mlp._layers[:-1]:\n",
    "            self.assertTrue(((layer > -1) & (layer < 1)).all())\n",
    "        self.assertTrue((mlp._layers[-1] == 0).all())\n",
    "\n",
    "    def test_single_predict(self):\n",
    "        mlp = MLP([2, 3, 2], [lambda x: x, lambda x: 2*x, lambda x: x], [lambda x: 1, lambda x: 2, lambda x: 1], 2)\n",
    "        mlp.initialize_weights('ones')\n",
    "        self.assertEqual(mlp.propagate([2,  3]), [79, 79])\n",
    "\n",
    "    def test_backprop(self):\n",
    "        mlp = MLP([2, 3, 1], [lambda x: x**2, lambda x: x**2, lambda x: x], [lambda x: 2*x, lambda x: 2*x, lambda x: 1], 1)\n",
    "        mlp.initialize_weights('ones') \n",
    "        gradient = mlp.backprop([3], [3000], lambda t, v: 2*(t - v))\n",
    "        true_gradient = np.array([np.array([[2547072.,  849024.],\n",
    "                                   [2547072.,  849024.]]),\n",
    "                         np.array([[566016., 566016.,  35376.],\n",
    "                                   [566016., 566016.,  35376.],\n",
    "                                   [566016., 566016.,  35376.]]),\n",
    "                         np.array([[5.83704e+05, 5.83704e+05, 5.83704e+05, 5.36000e+02]])], dtype=object)\n",
    "        self.assertTrue(all([(ar1 == ar2).all() for ar1, ar2 in zip (gradient, true_gradient)]))\n",
    "\n",
    "unittest.main(argv=[''],  exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "func = lambda x: x**2\n",
    "data = pd.DataFrame(data=[[x, func(x)] for x in RNG.uniform(-1, 1, 1000)], columns=['x', 'y'])\n",
    "x_train, x_test, y_train, y_test = train_test_split(data['x'], data['y'], test_size=0.2, random_state=32)\n",
    "train = x_train.to_frame().join(y_train.to_frame())\n",
    "test = x_test.to_frame().join(y_test.to_frame())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1793df1d2301171ba0b40f65791b7f70118724e366e1aedbfbefba442cbae2c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
